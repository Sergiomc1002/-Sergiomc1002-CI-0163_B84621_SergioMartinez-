{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 238\n",
      "1 297\n",
      "2 927\n",
      "3 933\n",
      "4 179\n",
      "5 375\n",
      "6 820\n",
      "7 618\n",
      "8 561\n",
      "9 57\n",
      "10 577\n",
      "11 59\n",
      "12 73\n",
      "13 107\n",
      "14 53\n",
      "15 91\n",
      "16 893\n",
      "17 810\n",
      "18 170\n",
      "19 53\n",
      "20 68\n",
      "21 9\n",
      "22 1\n",
      "23 92\n",
      "24 9\n",
      "25 8\n",
      "26 9\n",
      "27 308\n",
      "28 447\n",
      "29 392\n",
      "30 107\n",
      "31 42\n",
      "32 4\n",
      "33 45\n",
      "34 141\n",
      "35 110\n",
      "36 3\n",
      "37 758\n",
      "38 9\n",
      "39 9\n",
      "40 388\n",
      "41 220\n",
      "42 644\n",
      "43 649\n",
      "44 499\n",
      "45 2\n",
      "46 937\n",
      "47 169\n",
      "48 286\n",
      "49 2\n"
     ]
    }
   ],
   "source": [
    "# summarize the number of unique values for each column using numpy\n",
    "from urllib.request import urlopen\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "from numpy import *\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "data = loadtxt(urlopen(path), delimiter=',')\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "\tprint(i, len(unique(data[:, i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     238\n",
      "1     297\n",
      "2     927\n",
      "3     933\n",
      "4     179\n",
      "5     375\n",
      "6     820\n",
      "7     618\n",
      "8     561\n",
      "9      57\n",
      "10    577\n",
      "11     59\n",
      "12     73\n",
      "13    107\n",
      "14     53\n",
      "15     91\n",
      "16    893\n",
      "17    810\n",
      "18    170\n",
      "19     53\n",
      "20     68\n",
      "21      9\n",
      "22      1\n",
      "23     92\n",
      "24      9\n",
      "25      8\n",
      "26      9\n",
      "27    308\n",
      "28    447\n",
      "29    392\n",
      "30    107\n",
      "31     42\n",
      "32      4\n",
      "33     45\n",
      "34    141\n",
      "35    110\n",
      "36      3\n",
      "37    758\n",
      "38      9\n",
      "39      9\n",
      "40    388\n",
      "41    220\n",
      "42    644\n",
      "43    649\n",
      "44    499\n",
      "45      2\n",
      "46    937\n",
      "47    169\n",
      "48    286\n",
      "49      2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# summarize the number of unique values for each column using numpy\n",
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "# summarize the number of unique values in each column\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n",
      "[22]\n",
      "(937, 49)\n"
     ]
    }
   ],
   "source": [
    "# delete columns with a single unique value\n",
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "print(df.shape)\n",
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if v == 1]\n",
    "print(to_del)\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 238, 25.4%\n",
      "1, 297, 31.7%\n",
      "2, 927, 98.9%\n",
      "3, 933, 99.6%\n",
      "4, 179, 19.1%\n",
      "5, 375, 40.0%\n",
      "6, 820, 87.5%\n",
      "7, 618, 66.0%\n",
      "8, 561, 59.9%\n",
      "9, 57, 6.1%\n",
      "10, 577, 61.6%\n",
      "11, 59, 6.3%\n",
      "12, 73, 7.8%\n",
      "13, 107, 11.4%\n",
      "14, 53, 5.7%\n",
      "15, 91, 9.7%\n",
      "16, 893, 95.3%\n",
      "17, 810, 86.4%\n",
      "18, 170, 18.1%\n",
      "19, 53, 5.7%\n",
      "20, 68, 7.3%\n",
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "23, 92, 9.8%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "27, 308, 32.9%\n",
      "28, 447, 47.7%\n",
      "29, 392, 41.8%\n",
      "30, 107, 11.4%\n",
      "31, 42, 4.5%\n",
      "32, 4, 0.4%\n",
      "33, 45, 4.8%\n",
      "34, 141, 15.0%\n",
      "35, 110, 11.7%\n",
      "36, 3, 0.3%\n",
      "37, 758, 80.9%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "40, 388, 41.4%\n",
      "41, 220, 23.5%\n",
      "42, 644, 68.7%\n",
      "43, 649, 69.3%\n",
      "44, 499, 53.3%\n",
      "45, 2, 0.2%\n",
      "46, 937, 100.0%\n",
      "47, 169, 18.0%\n",
      "48, 286, 30.5%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "# summarize the percentage of unique values for each column using numpy\n",
    "from urllib.request import urlopen\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "data = loadtxt(urlopen(path), delimiter=',')\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "\tnum = len(unique(data[:, i]))\n",
    "\tpercentage = float(num) / data.shape[0] * 100\n",
    "\tprint('%d, %d, %.1f%%' % (i, num, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21, 9, 1.0%\n",
      "22, 1, 0.1%\n",
      "24, 9, 1.0%\n",
      "25, 8, 0.9%\n",
      "26, 9, 1.0%\n",
      "32, 4, 0.4%\n",
      "36, 3, 0.3%\n",
      "38, 9, 1.0%\n",
      "39, 9, 1.0%\n",
      "45, 2, 0.2%\n",
      "49, 2, 0.2%\n"
     ]
    }
   ],
   "source": [
    "# summarize the percentage of unique values for each column using numpy\n",
    "from urllib.request import urlopen\n",
    "from numpy import loadtxt\n",
    "from numpy import unique\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "data = loadtxt(urlopen(path), delimiter=',')\n",
    "# summarize the number of unique values in each column\n",
    "for i in range(data.shape[1]):\n",
    "\tnum = len(unique(data[:, i]))\n",
    "\tpercentage = float(num) / data.shape[0] * 100\n",
    "\tif percentage < 1:\n",
    "\t\tprint('%d, %d, %.1f%%' % (i, num, percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50)\n",
      "[21, 22, 24, 25, 26, 32, 36, 38, 39, 45, 49]\n",
      "(937, 39)\n"
     ]
    }
   ],
   "source": [
    "# delete columns where number of unique values is less than 1% of the rows\n",
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "print(df.shape)\n",
    "# get number of unique values for each column\n",
    "counts = df.nunique()\n",
    "# record columns to delete\n",
    "to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0]*100) < 1]\n",
    "print(to_del)\n",
    "# drop useless columns\n",
    "df.drop(to_del, axis=1, inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 49) (937,)\n",
      "(937, 48)\n"
     ]
    }
   ],
   "source": [
    "# example of apply the variance threshold\n",
    "from pandas import read_csv\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/oil-spill.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "# split data into inputs and outputs\n",
    "data = df.values\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "# define the transform\n",
    "transform = VarianceThreshold()\n",
    "# transform the input data\n",
    "X_sel = transform.fit_transform(X)\n",
    "print(X_sel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Threshold=0.00, Features=48\n",
      ">Threshold=0.05, Features=37\n",
      ">Threshold=0.10, Features=36\n",
      ">Threshold=0.15, Features=35\n",
      ">Threshold=0.20, Features=35\n",
      ">Threshold=0.25, Features=35\n",
      ">Threshold=0.30, Features=35\n",
      ">Threshold=0.35, Features=35\n",
      ">Threshold=0.40, Features=35\n",
      ">Threshold=0.45, Features=33\n",
      ">Threshold=0.50, Features=31\n"
     ]
    }
   ],
   "source": [
    "...\n",
    "# apply transform with each threshold\n",
    "thresholds = arange(0.0, 0.55, 0.05)\n",
    "results = list()\n",
    "for t in thresholds:\n",
    "\t# define the transform\n",
    "\ttransform = VarianceThreshold(threshold=t)\n",
    "\t# transform the input data\n",
    "\tX_sel = transform.fit_transform(X)\n",
    "\t# determine the number of input features\n",
    "\tn_features = X_sel.shape[1]\n",
    "\tprint('>Threshold=%.2f, Features=%d' % (t, n_features))\n",
    "\t# store the result\n",
    "\tresults.append(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "       0    1    2    3               4\n",
      "34   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "37   4.9  3.1  1.5  0.1     Iris-setosa\n",
      "142  5.8  2.7  5.1  1.9  Iris-virginica\n"
     ]
    }
   ],
   "source": [
    "# locate rows of duplicate data\n",
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "# calculate duplicates\n",
    "dups = df.duplicated()\n",
    "# report if there are any duplicates\n",
    "print(dups.any())\n",
    "# list all duplicate rows\n",
    "print(df[dups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 5)\n",
      "(147, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# delete rows of duplicate data from the dataset\n",
    "from pandas import read_csv\n",
    "# define the location of the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
    "# load the dataset\n",
    "df = read_csv(path, header=None)\n",
    "print(df.shape)\n",
    "# delete duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comentarios del ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este tutorial enseñó bastantes comandos que se ve serán bastante usasdos a la hora de necesitar hacer un data cleaning en un set de datos, la verdad nunca lo había trabajado de esta manera entonces poder aprenderlo de manera práctica me gusto mucho y siento que es más útil que solo leerlo. Siento que voy a estar volviendo a este tutorial en el futuro cuando necesite hacer un data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
